{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder architecture\n",
    "---\n",
    "This is the second notebook, that heavily bases on the encoder implementation. All the comments are in the `encoder.ipynb`, and you should start learning from there.\n",
    "\n",
    "There are two main differences here:\n",
    "- `CasualHeadAttention` is a version of the `MultiHeadAttention` class that contains `casual_mask`. This mask is a matrix with all the upper right triangle values equal to 0, and it is applied on the entire input sequence. The general goal is to ensure that decoder can only see the words before the word that is currently analyzed, so for example in word 4, decoder sees only words 1, 2, 3, and 4.\n",
    "- `Decoder` class differs from the `Encoder` class in terms of output size. The output size is $T \\times DictionarySize$. For example, if the longest sentence in the batch contained 30 words, and the dictionary contains 20,000 words, the network returns $30 \\times 20,000$ matrix (30 words, each word one-hot-encoded).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualHeadAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "        \n",
    "        self.out = nn.Linear(d_k * n_heads, d_model)\n",
    "        \n",
    "        # Casual mask \n",
    "        cm = torch.tril(torch.ones(max_len, max_len))\n",
    "        self.register_buffer(\n",
    "            'casual_mask',\n",
    "            cm.view(1, 1, max_len, max_len)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, q, k, v, pad_mask=None):\n",
    "        \n",
    "        # Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "        \n",
    "        q = self.query(q) # N x T x (h*d_k) \n",
    "        k = self.key(k)   # N x T x (h*d_k)\n",
    "        v = self.value(v) # N x T x (h*d_v) # d_v == d_k\n",
    "        \n",
    "        N = q.shape[0] # batch size\n",
    "        T = q.shape[1] # sequence length\n",
    "        \n",
    "        # Changing shapes (reuqired for matrix multiplication)\n",
    "        # view: (N, T, h*d_k) -> (N, T, h, d_k)\n",
    "        # transpose: (N, T, h, d_k) -> (N, h, T, d_k)\n",
    "        \n",
    "        q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # (N, h, T, d_k) x (N, h, d_k, T) -> (N, h, T, T)\n",
    "        atention_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if pad_mask is not None:\n",
    "            # Mask has (N, T) shape, so we need to add two (inner) dimensions\n",
    "            # We also change zeros with -inf, so that softmax will ignore these values\n",
    "            atention_scores = atention_scores.masked_fill(\n",
    "                 pad_mask[:, None, None, :] == 0, float('-inf')\n",
    "                 )\n",
    "            \n",
    "        # We also need to add casual mask, so that we don't look into the future\n",
    "        # Max_len is the length of the longest sequence possible, but in fact,\n",
    "        # we need the longest sequence in the batch. Thus we crop casual mask to :T size            \n",
    "        atention_scores = atention_scores.masked_fill(\n",
    "                self.casual_mask[:, :, :T, :T] == 0, float('-inf')\n",
    "                )\n",
    "        \n",
    "        attention_weights = F.softmax(atention_scores, dim=-1)\n",
    "        \n",
    "        A = attention_weights @ v\n",
    "        \n",
    "        # Reshape (N, h, T, d_k) -> (N, T, h, d_k) -> (N, T, h*d_k)\n",
    "        A = A.transpose(1, 2)\n",
    "        \n",
    "        # Concatenate\n",
    "        A = A.contiguous().view(N, T, self.n_heads * self.d_k)\n",
    "        \n",
    "        return self.out(A)\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.attention = CasualHeadAttention(d_k, d_model, n_heads, max_len)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.norm1(x + self.attention(x, x, x, pad_mask))\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # [ [0], [1], [2], ..., [max_len-1] ]\n",
    "        # 2d array of size max_len x 1\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #[0, 2, 4, ...]\n",
    "        exp_term = torch.arange(0, d_model, 2) \n",
    "        \n",
    "        \n",
    "        div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        \n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape: N x T x D\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_len,\n",
    "        d_k,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout=dropout)\n",
    "        transformer_blocks = [\n",
    "            TransformerBlock(d_k, d_model, n_heads, max_len, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, pad_mask = None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, pad_mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(20000, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): CasualHeadAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): CasualHeadAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=64, out_features=20000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Decoder(\n",
    "    vocab_size=20_000,\n",
    "    max_len = 1024,\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 2,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 20000])\n",
      "torch.Size([8, 512, 20000])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "nr_words = 512\n",
    "x = np.random.randint(0, 20_000, size=(batch_size, nr_words))\n",
    "x_t = torch.tensor(x).to(device)\n",
    "\n",
    "mask = np.ones((batch_size, nr_words))\n",
    "mask[:, 256:] = 0\n",
    "mask_t = torch.tensor(mask).to(device)\n",
    "\n",
    "# Without mask\n",
    "y = model(x_t)\n",
    "print (y.shape)\n",
    "\n",
    "# With mask\n",
    "y = model(x_t, mask_t)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
