{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder architecture\n",
    "---\n",
    "This is the second notebook, that heavily bases on the encoder implementation. All the comments are in the `encoder.ipynb`, and you should start learning from there.\n",
    "\n",
    "There are two main differences here:\n",
    "- `CausalHeadAttention` is a version of the `MultiHeadAttention` class that contains `causal_mask`. This mask is a matrix with all the upper right triangle values equal to 0, and it is applied on the entire input sequence. The general goal is to ensure that decoder can only see the words before the word that is currently analyzed, so for example in word 4, decoder sees only words 1, 2, 3, and 4.\n",
    "- `Decoder` class differs from the `Encoder` class in terms of output size. The output size is $T \\times DictionarySize$. For example, if the longest sentence in the batch contained 30 words, and the dictionary contains 20,000 words, the network returns $30 \\times 20,000$ matrix (30 words, each word one-hot-encoded).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalHeadAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "        \n",
    "        self.out = nn.Linear(d_k * n_heads, d_model)\n",
    "        \n",
    "        # Causal mask \n",
    "        cm = torch.tril(torch.ones(max_len, max_len))\n",
    "        self.register_buffer(\n",
    "            'causal_mask',\n",
    "            cm.view(1, 1, max_len, max_len)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, q, k, v, pad_mask=None):\n",
    "        \n",
    "        # Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "        \n",
    "        q = self.query(q) # N x T x (h*d_k) \n",
    "        k = self.key(k)   # N x T x (h*d_k)\n",
    "        v = self.value(v) # N x T x (h*d_v) # d_v == d_k\n",
    "        \n",
    "        N = q.shape[0] # batch size\n",
    "        T = q.shape[1] # sequence length\n",
    "        \n",
    "        # Changing shapes (reuqired for matrix multiplication)\n",
    "        # view: (N, T, h*d_k) -> (N, T, h, d_k)\n",
    "        # transpose: (N, T, h, d_k) -> (N, h, T, d_k)\n",
    "        \n",
    "        q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # (N, h, T, d_k) x (N, h, d_k, T) -> (N, h, T, T)\n",
    "        atention_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if pad_mask is not None:\n",
    "            # Mask has (N, T) shape, so we need to add two (inner) dimensions\n",
    "            # We also change zeros with -inf, so that softmax will ignore these values\n",
    "            atention_scores = atention_scores.masked_fill(\n",
    "                 pad_mask[:, None, None, :] == 0, float('-inf')\n",
    "                 )\n",
    "            \n",
    "        # We also need to add causal mask, so that we don't look into the future\n",
    "        # Max_len is the length of the longest sequence possible, but in fact,\n",
    "        # we need the longest sequence in the batch. Thus we crop causal mask to :T size            \n",
    "        atention_scores = atention_scores.masked_fill(\n",
    "                self.causal_mask[:, :, :T, :T] == 0, float('-inf')\n",
    "                )\n",
    "        \n",
    "        attention_weights = F.softmax(atention_scores, dim=-1)\n",
    "        \n",
    "        A = attention_weights @ v\n",
    "        \n",
    "        # Reshape (N, h, T, d_k) -> (N, T, h, d_k) -> (N, T, h*d_k)\n",
    "        A = A.transpose(1, 2)\n",
    "        \n",
    "        # Concatenate\n",
    "        A = A.contiguous().view(N, T, self.n_heads * self.d_k)\n",
    "        \n",
    "        return self.out(A)\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.attention = CausalHeadAttention(d_k, d_model, n_heads, max_len)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),            \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.norm1(x + self.attention(x, x, x, pad_mask))\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # [ [0], [1], [2], ..., [max_len-1] ]\n",
    "        # 2d array of size max_len x 1\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #[0, 2, 4, ...]\n",
    "        exp_term = torch.arange(0, d_model, 2) \n",
    "        \n",
    "        \n",
    "        div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        \n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape: N x T x D\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_len,\n",
    "        d_k,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout=dropout)\n",
    "        transformer_blocks = [\n",
    "            TransformerBlock(d_k, d_model, n_heads, max_len, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, pad_mask = None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, pad_mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CausalHeadAttention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m Decoder(\n\u001b[1;32m      2\u001b[0m     vocab_size\u001b[39m=\u001b[39;49m\u001b[39m20_000\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     max_len \u001b[39m=\u001b[39;49m \u001b[39m1024\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     d_k \u001b[39m=\u001b[39;49m \u001b[39m16\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     d_model \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     n_heads \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     n_layers \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     dropout \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m (device)\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[0;34m(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, dropout)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(vocab_size, d_model)\n\u001b[1;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoding \u001b[39m=\u001b[39m PositionalEncoding(d_model, max_len, dropout\u001b[39m=\u001b[39mdropout)\n\u001b[0;32m---> 17\u001b[0m transformer_blocks \u001b[39m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     TransformerBlock(d_k, d_model, n_heads, max_len, dropout\u001b[39m=\u001b[39;49mdropout)\n\u001b[1;32m     19\u001b[0m     \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n_layers)\n\u001b[1;32m     20\u001b[0m ]\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39mtransformer_blocks)\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(d_model)\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(vocab_size, d_model)\n\u001b[1;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoding \u001b[39m=\u001b[39m PositionalEncoding(d_model, max_len, dropout\u001b[39m=\u001b[39mdropout)\n\u001b[1;32m     17\u001b[0m transformer_blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m---> 18\u001b[0m     TransformerBlock(d_k, d_model, n_heads, max_len, dropout\u001b[39m=\u001b[39;49mdropout)\n\u001b[1;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_layers)\n\u001b[1;32m     20\u001b[0m ]\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39mtransformer_blocks)\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(d_model)\n",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[0;34m(self, d_k, d_model, n_heads, max_len, dropout)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, d_k, d_model, n_heads, max_len, dropout\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m CausalHeadAttention(d_k, d_model, n_heads, max_len)\n\u001b[1;32m      6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(d_model)\n\u001b[1;32m      7\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(d_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CausalHeadAttention' is not defined"
     ]
    }
   ],
   "source": [
    "model = Decoder(\n",
    "    vocab_size=20_000,\n",
    "    max_len = 1024,\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 2,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "nr_words = 512\n",
    "x = np.random.randint(0, 20_000, size=(batch_size, nr_words))\n",
    "x_t = torch.tensor(x).to(device)\n",
    "\n",
    "mask = np.ones((batch_size, nr_words))\n",
    "mask[:, 256:] = 0\n",
    "mask_t = torch.tensor(mask).to(device)\n",
    "\n",
    "# Without mask\n",
    "y = model(x_t)\n",
    "print (y.shape)\n",
    "\n",
    "# With mask\n",
    "y = model(x_t, mask_t)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "checkpoint = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['sentence'], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"sentence\", \"label\", \"idx\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=32, \n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print('k:', k, 'n.shape:', v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use padding token id to tell the CrossEntropyLoss \n",
    "# to ignore the padding token in the input sequence.\n",
    "print ('Padding token:', tokenizer.pad_token)\n",
    "print ('Padding token id:', tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len = tokenizer.max_model_input_sizes[checkpoint],\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 2,\n",
    "    dropout = 0.1\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "Here is an extra code that uses the model to predict the outputs. \n",
    "We are going to test the model behaviors after each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, device, prompt = \"I'm \", max_output_length = 160):\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = tokenized_prompt[\"input_ids\"][:, :-1].to(device)\n",
    "        mask = tokenized_prompt[\"attention_mask\"][:, :-1].to(device)\n",
    "\n",
    "        for _ in range (max_output_length):\n",
    "            outputs = model(input_ids, mask)\n",
    "            prediction_id = torch.argmax(outputs[:, -1, :], axis=-1)\n",
    "\n",
    "            input_ids = torch.hstack((input_ids, prediction_id[:, None]))\n",
    "            mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "            if prediction_id == tokenizer.sep_token_id:\n",
    "                break\n",
    "\n",
    "        return tokenizer.decode(input_ids[0])\n",
    "    \n",
    "generate(model, tokenizer, device, max_output_length = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, epochs, valid_loader = None, print_every = 1):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    valid_losses = np.zeros(epochs)\n",
    "    \n",
    "    for epoch in range (epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        print ('Training...    \\r', end = '')\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Targets are just inputs, but shifted by one position (backward)\n",
    "            targets = batch['input_ids'].clone().detach()\n",
    "            targets = torch.roll(targets, shifts=-1, dims = -1)\n",
    "            targets[:, -1] = tokenizer.pad_token_id\n",
    "            \n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            \n",
    "            # This part is tricky. Our output shape is N x T x V, \n",
    "            # where N is batch size, T is sequence length, and V is vocab size,\n",
    "            # and our targets shape is N x T.\n",
    "            # CrossEntropyLoss expects scores in the form \n",
    "            # N x V x T, so we need to transpose            \n",
    "            loss = criterion(outputs.transpose(2, 1), targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "\n",
    "        if valid_loader is not None:\n",
    "            print ('Validating...    \\r', end = '')\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            n_valid = 0\n",
    "            for batch in valid_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                targets = batch['input_ids'].clone().detach()\n",
    "                targets = torch.roll(targets, shifts=-1, dims = -1)\n",
    "                targets[:, -1] = tokenizer.pad_token_id\n",
    "            \n",
    "                outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "                loss = criterion(outputs.transpose(2, 1), targets)                \n",
    "                    \n",
    "                valid_loss += loss.item()*batch[\"input_ids\"].size(0)\n",
    "                n_valid += batch[\"input_ids\"].size(0)\n",
    "            \n",
    "            valid_loss /= n_valid\n",
    "        else:\n",
    "            valid_loss = np.nan\n",
    "        \n",
    "        train_losses[epoch] = train_loss\n",
    "        valid_losses[epoch] = valid_loss\n",
    "\n",
    "        generated_text = generate(model, tokenizer, device, prompt = \"I'm \", max_output_length = 10)\n",
    " \n",
    "        if epoch%print_every == 0:\n",
    "            t1 = datetime.now() - t0\n",
    "            minutes, seconds = divmod(t1.total_seconds(), 60)\n",
    "            formatted_time = \"{:02}:{:02}\".format(int(minutes), int(seconds))\n",
    "            \n",
    "            \n",
    "            print (f'Epoch: {epoch}: Train loss: {train_loss:.2f}, Valid loss: {valid_loss:.2f}, Duration: {formatted_time}min, Text: {generated_text}')\n",
    "           \n",
    "           \n",
    "    return train_losses, valid_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses = train(\n",
    "    model=model, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer,   \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "one_elem_loader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "for batch in one_elem_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    prediction_ids = torch.argmax(outputs, axis=-1)\n",
    "    break\n",
    "\n",
    "print (outputs.shape)\n",
    "print (prediction_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Input:', tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "print ('Output:', tokenizer.decode(prediction_ids[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, device, prompt = \"I'm \"))\n",
    "print ('\\n')\n",
    "print (generate(model, tokenizer, device, prompt = \"Transformers are \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
