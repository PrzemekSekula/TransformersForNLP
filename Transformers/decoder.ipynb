{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder architecture\n",
    "---\n",
    "This is the second notebook, that heavily bases on the encoder implementation. All the comments are in the `encoder.ipynb`, and you should start learning from there.\n",
    "\n",
    "There are two main differences here:\n",
    "- `CasualHeadAttention` is a version of the `MultiHeadAttention` class that contains `casual_mask`. This mask is a matrix with all the upper right triangle values equal to 0, and it is applied on the entire input sequence. The general goal is to ensure that decoder can only see the words before the word that is currently analyzed, so for example in word 4, decoder sees only words 1, 2, 3, and 4.\n",
    "- `Decoder` class differs from the `Encoder` class in terms of output size. The output size is $T \\times DictionarySize$. For example, if the longest sentence in the batch contained 30 words, and the dictionary contains 20,000 words, the network returns $30 \\times 20,000$ matrix (30 words, each word one-hot-encoded).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualHeadAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "        \n",
    "        self.out = nn.Linear(d_k * n_heads, d_model)\n",
    "        \n",
    "        # Casual mask \n",
    "        cm = torch.tril(torch.ones(max_len, max_len))\n",
    "        self.register_buffer(\n",
    "            'casual_mask',\n",
    "            cm.view(1, 1, max_len, max_len)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, q, k, v, pad_mask=None):\n",
    "        \n",
    "        # Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "        \n",
    "        q = self.query(q) # N x T x (h*d_k) \n",
    "        k = self.key(k)   # N x T x (h*d_k)\n",
    "        v = self.value(v) # N x T x (h*d_v) # d_v == d_k\n",
    "        \n",
    "        N = q.shape[0] # batch size\n",
    "        T = q.shape[1] # sequence length\n",
    "        \n",
    "        # Changing shapes (reuqired for matrix multiplication)\n",
    "        # view: (N, T, h*d_k) -> (N, T, h, d_k)\n",
    "        # transpose: (N, T, h, d_k) -> (N, h, T, d_k)\n",
    "        \n",
    "        q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # (N, h, T, d_k) x (N, h, d_k, T) -> (N, h, T, T)\n",
    "        atention_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if pad_mask is not None:\n",
    "            # Mask has (N, T) shape, so we need to add two (inner) dimensions\n",
    "            # We also change zeros with -inf, so that softmax will ignore these values\n",
    "            atention_scores = atention_scores.masked_fill(\n",
    "                 pad_mask[:, None, None, :] == 0, float('-inf')\n",
    "                 )\n",
    "            \n",
    "        # We also need to add casual mask, so that we don't look into the future\n",
    "        # Max_len is the length of the longest sequence possible, but in fact,\n",
    "        # we need the longest sequence in the batch. Thus we crop casual mask to :T size            \n",
    "        atention_scores = atention_scores.masked_fill(\n",
    "                self.casual_mask[:, :, :T, :T] == 0, float('-inf')\n",
    "                )\n",
    "        \n",
    "        attention_weights = F.softmax(atention_scores, dim=-1)\n",
    "        \n",
    "        A = attention_weights @ v\n",
    "        \n",
    "        # Reshape (N, h, T, d_k) -> (N, T, h, d_k) -> (N, T, h*d_k)\n",
    "        A = A.transpose(1, 2)\n",
    "        \n",
    "        # Concatenate\n",
    "        A = A.contiguous().view(N, T, self.n_heads * self.d_k)\n",
    "        \n",
    "        return self.out(A)\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.attention = CasualHeadAttention(d_k, d_model, n_heads, max_len)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.norm1(x + self.attention(x, x, x, pad_mask))\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # [ [0], [1], [2], ..., [max_len-1] ]\n",
    "        # 2d array of size max_len x 1\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #[0, 2, 4, ...]\n",
    "        exp_term = torch.arange(0, d_model, 2) \n",
    "        \n",
    "        \n",
    "        div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        \n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape: N x T x D\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_len,\n",
    "        d_k,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout=dropout)\n",
    "        transformer_blocks = [\n",
    "            TransformerBlock(d_k, d_model, n_heads, max_len, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, pad_mask = None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, pad_mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(20000, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): CasualHeadAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): CasualHeadAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=64, out_features=20000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Decoder(\n",
    "    vocab_size=20_000,\n",
    "    max_len = 1024,\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 2,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 20000])\n",
      "torch.Size([8, 512, 20000])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "nr_words = 512\n",
    "x = np.random.randint(0, 20_000, size=(batch_size, nr_words))\n",
    "x_t = torch.tensor(x).to(device)\n",
    "\n",
    "mask = np.ones((batch_size, nr_words))\n",
    "mask[:, 256:] = 0\n",
    "mask_t = torch.tensor(mask).to(device)\n",
    "\n",
    "# Without mask\n",
    "y = model(x_t)\n",
    "print (y.shape)\n",
    "\n",
    "# With mask\n",
    "y = model(x_t, mask_t)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "checkpoint = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['sentence'], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"sentence\", \"label\", \"idx\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=32, \n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: input_ids n.shape: torch.Size([32, 55])\n",
      "k: attention_mask n.shape: torch.Size([32, 55])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print('k:', k, 'n.shape:', v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token: [PAD]\n",
      "Padding token id: 0\n"
     ]
    }
   ],
   "source": [
    "# We will use padding token id to tell the CrossEntropyLoss \n",
    "# to ignore the padding token in the input sequence.\n",
    "print ('Padding token:', tokenizer.pad_token)\n",
    "print ('Padding token id:', tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(28996, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): CasualHeadAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): CasualHeadAttention(\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=64, out_features=28996, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Decoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len = tokenizer.max_model_input_sizes[checkpoint],\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 2,\n",
    "    dropout = 0.1\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "Here is an extra code that uses the model to predict the outputs. \n",
    "We are going to test the model behaviors after each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] I'm Extreme „ÅÆ gone vein economically Kemp traveling beverages cryingGC\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(model, tokenizer, device, prompt = \"I'm \", max_output_length = 160):\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = tokenized_prompt[\"input_ids\"][:, :-1].to(device)\n",
    "        mask = tokenized_prompt[\"attention_mask\"][:, :-1].to(device)\n",
    "\n",
    "        for _ in range (max_output_length):\n",
    "            outputs = model(input_ids, mask)\n",
    "            prediction_id = torch.argmax(outputs[:, -1, :], axis=-1)\n",
    "\n",
    "            input_ids = torch.hstack((input_ids, prediction_id[:, None]))\n",
    "            mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "            if prediction_id == tokenizer.sep_token_id:\n",
    "                break\n",
    "\n",
    "        return tokenizer.decode(input_ids[0])\n",
    "    \n",
    "generate(model, tokenizer, device, max_output_length = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, epochs, valid_loader = None, print_every = 1):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    valid_losses = np.zeros(epochs)\n",
    "    \n",
    "    for epoch in range (epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        print ('Training...    \\r', end = '')\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Targets are just inputs, but shifted by one position (backward)\n",
    "            targets = batch['input_ids'].clone().detach()\n",
    "            targets = torch.roll(targets, shifts=-1, dims = -1)\n",
    "            targets[:, -1] = tokenizer.pad_token_id\n",
    "            \n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            \n",
    "            # This part is tricky. Our output shape is N x T x V, \n",
    "            # where N is batch size, T is sequence length, and V is vocab size,\n",
    "            # and our targets shape is N x T.\n",
    "            # CrossEntropyLoss expects scores in the form \n",
    "            # N x V x T, so we need to transpose            \n",
    "            loss = criterion(outputs.transpose(2, 1), targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "\n",
    "        if valid_loader is not None:\n",
    "            print ('Validating...    \\r', end = '')\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            n_valid = 0\n",
    "            for batch in valid_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                targets = batch['input_ids'].clone().detach()\n",
    "                targets = torch.roll(targets, shifts=-1, dims = -1)\n",
    "                targets[:, -1] = tokenizer.pad_token_id\n",
    "            \n",
    "                outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "                loss = criterion(outputs.transpose(2, 1), targets)                \n",
    "                    \n",
    "                valid_loss += loss.item()*batch[\"input_ids\"].size(0)\n",
    "                n_valid += batch[\"input_ids\"].size(0)\n",
    "            \n",
    "            valid_loss /= n_valid\n",
    "        else:\n",
    "            valid_loss = np.nan\n",
    "        \n",
    "        train_losses[epoch] = train_loss\n",
    "        valid_losses[epoch] = valid_loss\n",
    "\n",
    "        generated_text = generate(model, tokenizer, device, prompt = \"I'm \", max_output_length = 10)\n",
    " \n",
    "        if epoch%print_every == 0:\n",
    "            t1 = datetime.now() - t0\n",
    "            minutes, seconds = divmod(t1.total_seconds(), 60)\n",
    "            formatted_time = \"{:02}:{:02}\".format(int(minutes), int(seconds))\n",
    "            \n",
    "            \n",
    "            print (f'Epoch: {epoch}: Train loss: {train_loss:.2f}, Valid loss: {valid_loss:.2f}, Duration: {formatted_time}min, Text: {generated_text}')\n",
    "           \n",
    "           \n",
    "    return train_losses, valid_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0: Train loss: 5.94, Valid loss: 5.71, Duration: 00:39min, Text: [CLS] I'm not be a movie [SEP]\n",
      "Epoch: 1: Train loss: 4.98, Valid loss: 5.70, Duration: 00:39min, Text: [CLS] I'm not only to be a few laughs, but it\n",
      "Epoch: 2: Train loss: 4.64, Valid loss: 5.75, Duration: 00:39min, Text: [CLS] I'm giving a movie that's a movie that is\n",
      "Epoch: 3: Train loss: 4.44, Valid loss: 5.78, Duration: 00:39min, Text: [CLS] I'm not a movie that is a movie that is a\n",
      "Epoch: 4: Train loss: 4.31, Valid loss: 5.84, Duration: 00:39min, Text: [CLS] I'm not a lot of a lot of a lot of\n",
      "Epoch: 5: Train loss: 4.20, Valid loss: 5.89, Duration: 00:39min, Text: [CLS] I'm not a movie that's been able to be\n",
      "Epoch: 6: Train loss: 4.10, Valid loss: 5.92, Duration: 00:39min, Text: [CLS] I'm giving the film's most sincere and the film\n",
      "Epoch: 7: Train loss: 4.02, Valid loss: 5.98, Duration: 00:39min, Text: [CLS] I'm not a movie that's a pianist, but\n",
      "Epoch: 8: Train loss: 3.95, Valid loss: 6.05, Duration: 00:39min, Text: [CLS] I'm not only to mention inappropriate and detract [SEP]\n",
      "Epoch: 9: Train loss: 3.90, Valid loss: 6.08, Duration: 00:39min, Text: [CLS] I'm not a very funny, but it's a\n",
      "Epoch: 10: Train loss: 3.84, Valid loss: 6.12, Duration: 00:39min, Text: [CLS] I'm not a movie that's not a good movie\n",
      "Epoch: 11: Train loss: 3.79, Valid loss: 6.15, Duration: 00:39min, Text: [CLS] I'm notwithstanding the writers'sporadic dips\n",
      "Epoch: 12: Train loss: 3.74, Valid loss: 6.15, Duration: 00:39min, Text: [CLS] I'm not a great missed opportunity to watch [SEP]\n",
      "Epoch: 13: Train loss: 3.70, Valid loss: 6.18, Duration: 00:39min, Text: [CLS] I'm not a hugely funny movie [SEP]\n",
      "Epoch: 14: Train loss: 3.66, Valid loss: 6.19, Duration: 00:39min, Text: [CLS] I'm not a good time believing people [SEP]\n",
      "Epoch: 15: Train loss: 3.63, Valid loss: 6.25, Duration: 00:39min, Text: [CLS] I'm not a movie that's apparently to be a\n",
      "Epoch: 16: Train loss: 3.59, Valid loss: 6.29, Duration: 00:39min, Text: [CLS] I'mollywood [SEP]\n",
      "Epoch: 17: Train loss: 3.56, Valid loss: 6.28, Duration: 00:39min, Text: [CLS] I'm not a great deal of a movie that's\n",
      "Epoch: 18: Train loss: 3.53, Valid loss: 6.30, Duration: 00:39min, Text: [CLS] I'm not a good movie [SEP]\n",
      "Epoch: 19: Train loss: 3.50, Valid loss: 6.34, Duration: 00:39min, Text: [CLS] I'm not a huge sacrifice of intelligence, but it '\n",
      "Epoch: 20: Train loss: 3.48, Valid loss: 6.36, Duration: 00:39min, Text: [CLS] I'm not a huge amount of humor [SEP]\n",
      "Epoch: 21: Train loss: 3.45, Valid loss: 6.37, Duration: 00:39min, Text: [CLS] I'm not a huge amount of a movie that's\n",
      "Epoch: 22: Train loss: 3.43, Valid loss: 6.40, Duration: 00:39min, Text: [CLS] I'm not a good movie [SEP]\n",
      "Epoch: 23: Train loss: 3.40, Valid loss: 6.43, Duration: 00:39min, Text: [CLS] I'm not a huge amount of the credit for the most\n",
      "Epoch: 24: Train loss: 3.38, Valid loss: 6.42, Duration: 00:39min, Text: [CLS] I'm not a participant [SEP]\n",
      "Epoch: 25: Train loss: 3.36, Valid loss: 6.45, Duration: 00:39min, Text: [CLS] I'm not a good movie [SEP]\n",
      "Epoch: 26: Train loss: 3.34, Valid loss: 6.48, Duration: 00:39min, Text: [CLS] I'm not a trace of humanity [SEP]\n",
      "Epoch: 27: Train loss: 3.32, Valid loss: 6.48, Duration: 00:39min, Text: [CLS] I'm not to mention inappropriate and wildly incongru\n",
      "Epoch: 28: Train loss: 3.31, Valid loss: 6.49, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 29: Train loss: 3.29, Valid loss: 6.51, Duration: 00:39min, Text: [CLS] I'm not a drunken driver through this movie. [SEP]\n",
      "Epoch: 30: Train loss: 3.27, Valid loss: 6.52, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 31: Train loss: 3.26, Valid loss: 6.55, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 32: Train loss: 3.24, Valid loss: 6.56, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 33: Train loss: 3.22, Valid loss: 6.56, Duration: 00:39min, Text: [CLS] I'm not a movie that's a good time [SEP]\n",
      "Epoch: 34: Train loss: 3.21, Valid loss: 6.56, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 35: Train loss: 3.20, Valid loss: 6.62, Duration: 00:39min, Text: [CLS] I'm not a good movie [SEP]\n",
      "Epoch: 36: Train loss: 3.18, Valid loss: 6.60, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 37: Train loss: 3.17, Valid loss: 6.65, Duration: 00:39min, Text: [CLS] I'm not to be awed by the worst of the\n",
      "Epoch: 38: Train loss: 3.16, Valid loss: 6.63, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 39: Train loss: 3.15, Valid loss: 6.67, Duration: 00:39min, Text: [CLS] I'm not a good movie [SEP]\n",
      "Epoch: 40: Train loss: 3.14, Valid loss: 6.68, Duration: 00:39min, Text: [CLS] I'm not a good movie [SEP]\n",
      "Epoch: 41: Train loss: 3.13, Valid loss: 6.71, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 42: Train loss: 3.12, Valid loss: 6.68, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 43: Train loss: 3.11, Valid loss: 6.68, Duration: 00:39min, Text: [CLS] I'm not a good movie [SEP]\n",
      "Epoch: 44: Train loss: 3.10, Valid loss: 6.70, Duration: 00:39min, Text: [CLS] I'm not sure to mention leaving the theater with a smile\n",
      "Epoch: 45: Train loss: 3.09, Valid loss: 6.71, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 46: Train loss: 3.08, Valid loss: 6.75, Duration: 00:39min, Text: [CLS] I'm not a classical dramatic impact [SEP]\n",
      "Epoch: 47: Train loss: 3.07, Valid loss: 6.74, Duration: 00:39min, Text: [CLS] I'm not a good time [SEP]\n",
      "Epoch: 48: Train loss: 3.06, Valid loss: 6.76, Duration: 00:39min, Text: [CLS] I'm not a good movie [SEP]\n",
      "Epoch: 49: Train loss: 3.05, Valid loss: 6.73, Duration: 00:39min, Text: [CLS] I'm not a cheat in the end [SEP]\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = train(\n",
    "    model=model, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer,   \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 28996])\n",
      "tensor([[ 170,  112,  188,  170, 2523, 1105, 9998, 6276, 2025,  102,  102,  102]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "one_elem_loader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "for batch in one_elem_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    prediction_ids = torch.argmax(outputs, axis=-1)\n",
    "    break\n",
    "\n",
    "print (outputs.shape)\n",
    "print (prediction_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [CLS] it's a charming and often affecting journey. [SEP]\n",
      "Output: a's a movie and intelligent funny study [SEP] [SEP] [SEP]\n"
     ]
    }
   ],
   "source": [
    "print ('Input:', tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "print ('Output:', tokenizer.decode(prediction_ids[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] I'm not a cheat in the end [SEP]\n",
      "\n",
      "\n",
      "[CLS] Transformers aren't a lioness trouble & tontollah in a cold mosque. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, device, prompt = \"I'm \"))\n",
    "print ('\\n')\n",
    "print (generate(model, tokenizer, device, prompt = \"Transformers are \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
