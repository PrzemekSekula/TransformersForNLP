{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder network implementation\n",
    "---\n",
    "The third notebook in the row. It assumes that we know how to build both Encoder and Decoder networks.\n",
    "\n",
    "The general idea is to create and combine two networks - endoder and decoder.\n",
    "After using N encoder bloks we take key `K` and `V` from the final block and inject them to every\n",
    "decoder block. In the decoder block query `Q` comes from the decoder, whereas `K` and `V` come from the encoder. We may interpret is as: how much the word `Q` (from translation) should pay attention to each words (`K`) in the sentence.\n",
    "\n",
    "\n",
    " <img src=\"./images/encoder-decoder.png\" alt=\"Attention and Multi-head Attention\" width=\"545\" />\n",
    " \n",
    "*Image from [Attention is All you need](https://arxiv.org/abs/1706.03762) paper*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention block\n",
    "\n",
    "`MultiHeadAttention` block should be more general this time. Specifically, `K` and `V` may come from another source than `Q`, and, as a result, have diferent shapes (diferent number of words). This is very natural, an English sentence and its Polish translation do not have to have the same number of words.\n",
    "We denote the length of the inputs as $T_{input}$, and the length of the outputs as  $T_{output}$. As a consequence, `K` and `V` have shapes  $T_{input} \\times d_k$ and `Q` has shape $T_{output} \\times d_k$.\n",
    "\n",
    "In fact, we are using $n_{heads}$ heads and $N$ batches, so the \"full\" shapes are:\n",
    "- for `K` and `V`: $N \\times n_{heads} \\times T_{input} \\times d_k$\n",
    "- for `Q`: $N \\times n_{heads} \\times T_{output} \\times d_k$\n",
    "\n",
    "The shapes are being changed in the code to keep the tensors squeezed or to make the tensor operations possible. Pay attention to the notes.\n",
    "\n",
    "Additionally, in the encoder-decoder structure, we will use two types of Multi-head attention blocks - with and without causal mask. The `causal` parameter takes care about it. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, causal=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "        \n",
    "        self.out = nn.Linear(d_k * n_heads, d_model)\n",
    "        \n",
    "        # Causal mask\n",
    "        self.causal = causal\n",
    "        if causal: \n",
    "            cm = torch.tril(torch.ones(max_len, max_len))\n",
    "            self.register_buffer(\n",
    "                'causal_mask',\n",
    "                cm.view(1, 1, max_len, max_len)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, q, k, v, pad_mask=None):\n",
    "        \n",
    "        # Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "        \n",
    "        q = self.query(q) # N x T_output x (h*d_k) \n",
    "        k = self.key(k)   # N x T_input x (h*d_k)\n",
    "        v = self.value(v) # N x T_input x (h*d_v) # d_v == d_k\n",
    "        \n",
    "        N = q.shape[0] # batch size\n",
    "        T_output = q.shape[1] # Sequence length q\n",
    "        T_input = k.shape[1] # Sequence length for k amd v\n",
    "        \n",
    "        # Changing shapes (reuqired for matrix multiplication)\n",
    "        # view: (N, T, h*d_k) -> (N, T, h, d_k)\n",
    "        # transpose: (N, T, h, d_k) -> (N, h, T, d_k)\n",
    "        q = q.view(N, T_output, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # (N, h, T_output, d_k) x (N, h, d_k, T_input) -> (N, h, T_output, T_input)\n",
    "        atention_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if pad_mask is not None:\n",
    "            # Mask has (N, T_input) shape, so we need to add two (inner) dimensions\n",
    "            # We also change zeros with -inf, so that softmax will ignore these values\n",
    "            atention_scores = atention_scores.masked_fill(\n",
    "                 pad_mask[:, None, None, :] == 0, float('-inf')\n",
    "                 )\n",
    "            \n",
    "        # We also may need to add cusal mask, so that we don't look into the future\n",
    "        # Max_len is the length of the longest sequence possible, but in fact,\n",
    "        # we need the longest sequence in the batch. Thus we crop causal mask to :T size      \n",
    "        # Moreover, after getting rid of first two dimensions (batch size and number of heads)\n",
    "        # Our mask number of rows corresponds to the Q sequence length, and number of columns\n",
    "        # corresponds to the K sequence length. Thus we need to crop the mask to the size of\n",
    "        # Q sequence length and K sequence length.\n",
    "        # Note 2: In the decoder part Q comes from the decoder, whereas K and V come from \n",
    "        # the encoder. We may interpret is as: how much the word Q (from translation)\n",
    "        # should pay attention to each words K in the sentence.  \n",
    "        if self.causal:      \n",
    "            atention_scores = atention_scores.masked_fill(\n",
    "                    self.causal_mask[:, :, :T_output, :T_input] == 0, float('-inf')\n",
    "                    )\n",
    "        \n",
    "        attention_weights = F.softmax(atention_scores, dim=-1)\n",
    "        \n",
    "        # (N, h, T_output, T_input) x (N, h, T_input, d_k) -> (N, h, T_output, d_k)\n",
    "        A = attention_weights @ v\n",
    "        \n",
    "        # Reshape (N, h, T_output, d_k) -> (N, T_output, h, d_k) -> (N, T_output, h*d_k)\n",
    "        A = A.transpose(1, 2)\n",
    "        A = A.contiguous().view(N, T_output, self.n_heads * self.d_k)\n",
    "        \n",
    "        return self.out(A)\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.attention = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.norm1(x + self.attention(x, x, x, pad_mask))\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.attention_1 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.attention_2 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "        x = self.norm1(dec_input + self.attention_1(dec_input, dec_input, dec_input, dec_mask))\n",
    "        \n",
    "        x = self.norm2(x + self.attention_2(x, enc_output, enc_output, enc_mask))\n",
    "        \n",
    "        x = self.norm3(x + self.ff(x))\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # [ [0], [1], [2], ..., [max_len-1] ]\n",
    "        # 2d array of size max_len x 1\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #[0, 2, 4, ...]\n",
    "        exp_term = torch.arange(0, d_model, 2) \n",
    "        \n",
    "        \n",
    "        div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        \n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape: N x T x D\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size : int,\n",
    "        max_len : int,\n",
    "        d_k : int,\n",
    "        d_model : int,\n",
    "        n_heads : int,\n",
    "        n_layers : int,\n",
    "        dropout : float = 0.1,\n",
    "    ):\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout=dropout)\n",
    "        transformer_blocks = [\n",
    "            EncoderBlock(d_k, d_model, n_heads, max_len, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        \n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, pad_mask = None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, pad_mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_len,\n",
    "        d_k,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout=dropout)\n",
    "        transformer_blocks = [\n",
    "            DecoderBlock(d_k, d_model, n_heads, max_len, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, enc_output, dec_input, enc_mask = None, dec_mask = None):\n",
    "        x = self.embedding(dec_input)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(enc_output, x, enc_mask, dec_mask)\n",
    "                \n",
    "        x = self.norm(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, enc_input, dec_input, enc_mask, dec_mask):\n",
    "        enc_output = self.encode(enc_input, enc_mask)\n",
    "        dec_output = self.decode(enc_output, dec_input, enc_mask, dec_mask)    \n",
    "        return dec_output\n",
    "    \n",
    "    def encode(self, enc_input, enc_mask):\n",
    "        return self.encoder(enc_input, enc_mask)\n",
    "    \n",
    "    def decode(self, enc_output, dec_input, enc_mask, dec_mask):\n",
    "        return self.decoder(enc_output, dec_input, enc_mask, dec_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocab_size=20_000,\n",
    "    max_len = 1024,\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 2,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size=10_000,\n",
    "    max_len = 1024,\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 2,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer = Transformer(encoder, decoder)\n",
    "transformer = transformer.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 10000])\n",
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "nr_words_enc = 512\n",
    "nr_words_dec = 256\n",
    "\n",
    "x = np.random.randint(0, 20_000, size=(batch_size, nr_words_enc))\n",
    "enc_input = torch.tensor(x).to(device)\n",
    "\n",
    "enc_mask = np.ones((batch_size, nr_words_enc))\n",
    "enc_mask[:, int(nr_words_enc/2):] = 0 # Let's cut off the second part of the sequence\n",
    "enc_mask = torch.tensor(enc_mask).to(device)\n",
    "\n",
    "x = np.random.randint(0, 10_000, size=(batch_size, nr_words_dec))\n",
    "dec_input = torch.tensor(x).to(device)\n",
    "dec_mask = np.ones((batch_size, nr_words_dec))\n",
    "dec_mask[:, int(nr_words_dec/2):] = 0 # Let's cut off the second part of the sequence\n",
    "dec_mask = torch.tensor(dec_mask).to(device)\n",
    "\n",
    "\n",
    "\n",
    "y = transformer(enc_input, dec_input, enc_mask, dec_mask)\n",
    "print (y.shape)\n",
    "print (y.argmax(dim=-1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
