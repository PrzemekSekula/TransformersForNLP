{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder network implementation\n",
    "---\n",
    "The third notebook in the row. It assumes that we know how to build both Encoder and Decoder networks.\n",
    "\n",
    "The general idea is to create and combine two networks - endoder and decoder.\n",
    "After using N encoder bloks we take key `K` and `V` from the final block and inject them to every\n",
    "decoder block. In the decoder block query `Q` comes from the decoder, whereas `K` and `V` come from the encoder. We may interpret is as: how much the word `Q` (from translation) should pay attention to each words (`K`) in the sentence.\n",
    "\n",
    "\n",
    " <img src=\"./images/encoder-decoder.png\" alt=\"Attention and Multi-head Attention\" width=\"545\" />\n",
    " \n",
    "*Image from [Attention is All you need](https://arxiv.org/abs/1706.03762) paper*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention block\n",
    "\n",
    "`MultiHeadAttention` block should be more general this time. Specifically, `K` and `V` may come from another source than `Q`, and, as a result, have diferent shapes (diferent number of words). This is very natural, an English sentence and its Polish translation do not have to have the same number of words.\n",
    "We denote the length of the inputs as $T_{input}$, and the length of the outputs as  $T_{output}$. As a consequence, `K` and `V` have shapes  $T_{input} \\times d_k$ and `Q` has shape $T_{output} \\times d_k$.\n",
    "\n",
    "In fact, we are using $n_{heads}$ heads and $N$ batches, so the \"full\" shapes are:\n",
    "- for `K` and `V`: $N \\times n_{heads} \\times T_{input} \\times d_k$\n",
    "- for `Q`: $N \\times n_{heads} \\times T_{output} \\times d_k$\n",
    "\n",
    "The shapes are being changed in the code to keep the tensors squeezed or to make the tensor operations possible. Pay attention to the notes.\n",
    "\n",
    "Additionally, in the encoder-decoder structure, we will use two types of Multi-head attention blocks - with and without causal mask. The `causal` parameter takes care about it. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, causal=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "        \n",
    "        self.out = nn.Linear(d_k * n_heads, d_model)\n",
    "        \n",
    "        # Causal mask\n",
    "        self.causal = causal\n",
    "        if causal: \n",
    "            cm = torch.tril(torch.ones(max_len, max_len))\n",
    "            self.register_buffer(\n",
    "                'causal_mask',\n",
    "                cm.view(1, 1, max_len, max_len)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, q, k, v, pad_mask=None):\n",
    "        \n",
    "        # Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "        \n",
    "        q = self.query(q) # N x T_output x (h*d_k) \n",
    "        k = self.key(k)   # N x T_input x (h*d_k)\n",
    "        v = self.value(v) # N x T_input x (h*d_v) # d_v == d_k\n",
    "        \n",
    "        N = q.shape[0] # batch size\n",
    "        T_output = q.shape[1] # Sequence length q\n",
    "        T_input = k.shape[1] # Sequence length for k amd v\n",
    "        \n",
    "        # Changing shapes (reuqired for matrix multiplication)\n",
    "        # view: (N, T, h*d_k) -> (N, T, h, d_k)\n",
    "        # transpose: (N, T, h, d_k) -> (N, h, T, d_k)\n",
    "        q = q.view(N, T_output, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(N, T_input, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # (N, h, T_output, d_k) x (N, h, d_k, T_input) -> (N, h, T_output, T_input)\n",
    "        atention_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if pad_mask is not None:\n",
    "            # Mask has (N, T_input) shape, so we need to add two (inner) dimensions\n",
    "            # We also change zeros with -inf, so that softmax will ignore these values\n",
    "            atention_scores = atention_scores.masked_fill(\n",
    "                 pad_mask[:, None, None, :] == 0, float('-inf')\n",
    "                 )\n",
    "            \n",
    "        # We also may need to add cusal mask, so that we don't look into the future\n",
    "        # Max_len is the length of the longest sequence possible, but in fact,\n",
    "        # we need the longest sequence in the batch. Thus we crop causal mask to :T size      \n",
    "        # Moreover, after getting rid of first two dimensions (batch size and number of heads)\n",
    "        # Our mask number of rows corresponds to the Q sequence length, and number of columns\n",
    "        # corresponds to the K sequence length. Thus we need to crop the mask to the size of\n",
    "        # Q sequence length and K sequence length.\n",
    "        # Note 2: In the decoder part Q comes from the decoder, whereas K and V come from \n",
    "        # the encoder. We may interpret is as: how much the word Q (from translation)\n",
    "        # should pay attention to each words K in the sentence.  \n",
    "        if self.causal:      \n",
    "            atention_scores = atention_scores.masked_fill(\n",
    "                    self.causal_mask[:, :, :T_output, :T_input] == 0, float('-inf')\n",
    "                    )\n",
    "        \n",
    "        attention_weights = F.softmax(atention_scores, dim=-1)\n",
    "        \n",
    "        # (N, h, T_output, T_input) x (N, h, T_input, d_k) -> (N, h, T_output, d_k)\n",
    "        A = attention_weights @ v\n",
    "        \n",
    "        # Reshape (N, h, T_output, d_k) -> (N, T_output, h, d_k) -> (N, T_output, h*d_k)\n",
    "        A = A.transpose(1, 2)\n",
    "        A = A.contiguous().view(N, T_output, self.n_heads * self.d_k)\n",
    "        \n",
    "        return self.out(A)\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.attention = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.norm1(x + self.attention(x, x, x, pad_mask))\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.attention_1 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.attention_2 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal=False)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, enc_output, dec_input, enc_mask=None, dec_mask=None):\n",
    "        x = self.norm1(dec_input + self.attention_1(dec_input, dec_input, dec_input, dec_mask))\n",
    "        \n",
    "        x = self.norm2(x + self.attention_2(x, enc_output, enc_output, enc_mask))\n",
    "        \n",
    "        x = self.norm3(x + self.ff(x))\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # [ [0], [1], [2], ..., [max_len-1] ]\n",
    "        # 2d array of size max_len x 1\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        #[0, 2, 4, ...]\n",
    "        exp_term = torch.arange(0, d_model, 2) \n",
    "        \n",
    "        \n",
    "        div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        \n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape: N x T x D\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size : int,\n",
    "        max_len : int,\n",
    "        d_k : int,\n",
    "        d_model : int,\n",
    "        n_heads : int,\n",
    "        n_layers : int,\n",
    "        dropout : float = 0.1,\n",
    "    ):\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout=dropout)\n",
    "        transformer_blocks = [\n",
    "            EncoderBlock(d_k, d_model, n_heads, max_len, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        \n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, pad_mask = None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, pad_mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_len,\n",
    "        d_k,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout=dropout)\n",
    "        transformer_blocks = [\n",
    "            DecoderBlock(d_k, d_model, n_heads, max_len, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, enc_output, dec_input, enc_mask = None, dec_mask = None):\n",
    "        x = self.embedding(dec_input)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(enc_output, x, enc_mask, dec_mask)\n",
    "                \n",
    "        x = self.norm(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, enc_input, dec_input, enc_mask, dec_mask):\n",
    "        enc_output = self.encode(enc_input, enc_mask)\n",
    "        dec_output = self.decode(enc_output, dec_input, enc_mask, dec_mask)    \n",
    "        return dec_output\n",
    "    \n",
    "    def encode(self, enc_input, enc_mask):\n",
    "        return self.encoder(enc_input, enc_mask)\n",
    "    \n",
    "    def decode(self, enc_output, dec_input, enc_mask, dec_mask):\n",
    "        return self.decoder(enc_output, dec_input, enc_mask, dec_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocab_size=20_000,\n",
    "    max_len = 1024,\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 2,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size=10_000,\n",
    "    max_len = 1024,\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 2,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer = Transformer(encoder, decoder)\n",
    "transformer = transformer.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "nr_words_enc = 512\n",
    "nr_words_dec = 256\n",
    "\n",
    "x = np.random.randint(0, 20_000, size=(batch_size, nr_words_enc))\n",
    "enc_input = torch.tensor(x).to(device)\n",
    "\n",
    "enc_mask = np.ones((batch_size, nr_words_enc))\n",
    "enc_mask[:, int(nr_words_enc/2):] = 0 # Let's cut off the second part of the sequence\n",
    "enc_mask = torch.tensor(enc_mask).to(device)\n",
    "\n",
    "x = np.random.randint(0, 10_000, size=(batch_size, nr_words_dec))\n",
    "dec_input = torch.tensor(x).to(device)\n",
    "dec_mask = np.ones((batch_size, nr_words_dec))\n",
    "dec_mask[:, int(nr_words_dec/2):] = 0 # Let's cut off the second part of the sequence\n",
    "dec_mask = torch.tensor(dec_mask).to(device)\n",
    "\n",
    "\n",
    "\n",
    "y = transformer(enc_input, dec_input, enc_mask, dec_mask)\n",
    "print (y.shape)\n",
    "print (y.argmax(dim=-1).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -nc https://lazyprogrammer.me/course_files/nlp3/spa.txt\n",
    "#!cat spa.txt | wc -l\n",
    "#!head spa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./spa.txt', sep='\\t', header=None, names=['en', 'es'])\n",
    "df.to_csv('./spa.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_length = df.en.apply(lambda x: len(x.split()))\n",
    "en_length.hist(bins=30)\n",
    "es_length = df.es.apply(lambda x: len(x.split()))\n",
    "es_length.hist(bins=30)\n",
    "\n",
    "print('en max length: ', en_length.max())\n",
    "print('es max length: ', es_length.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset(\"csv\", data_files='spa.csv')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = raw_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# We can use two different tokenizers for English and Spanish, or \n",
    "# we can use the same tokenizer for both languages. (We'll use the same tokenizer.)\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence = split['train'][0]['en']\n",
    "es_sentence = split['train'][0]['es']\n",
    "inputs = tokenizer(en_sentence)\n",
    "targets = tokenizer(text_target=es_sentence)\n",
    "print ('Inputs:', en_sentence, inputs)\n",
    "print ('Targets:', es_sentence, targets)\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(targets['input_ids'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    model_inputs = tokenizer(batch[\"en\"], truncation=True, max_length=max_input_length)\n",
    "    labels = tokenizer(batch[\"es\"], truncation=True, max_length=max_target_length)\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    # The masks for labels will be created in the training step\n",
    "    # This is because in fact we do not want to mask the labels, but the\n",
    "    # decoder inputs. And the decoder inputs are labels shifted to the right    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=split['train'].column_names,\n",
    ")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in batch.keys():\n",
    "    print (f\"{key}: {batch[key].shape}\")\n",
    "    \n",
    "print (batch['input_ids'])\n",
    "print (batch['attention_mask'])\n",
    "print (batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Special ids:', tokenizer.all_special_ids)\n",
    "print ('Special tokens:', tokenizer.all_special_tokens)\n",
    "print ('Vocab size:', tokenizer.vocab_size)\n",
    "tokenizer.decode([tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.unk_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have a start token, but we are going to need it. Thus, let's add our own token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'cls_token': '<s>'})\n",
    "print ('cls_token:', tokenizer.cls_token)\n",
    "print ('cls_token_id:', tokenizer.cls_token_id)\n",
    "print ('Vocab size:', tokenizer.vocab_size) # not updated, probably a bug\n",
    "print (tokenizer('<s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocab_size = tokenizer.vocab_size + 1,\n",
    "    max_len = 256,\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 3,\n",
    "    dropout = 0.1\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size = tokenizer.vocab_size + 1,\n",
    "    max_len = 512,\n",
    "    d_k = 16,\n",
    "    d_model = 64,\n",
    "    n_heads = 4,\n",
    "    n_layers = 3,\n",
    "    dropout = 0.1\n",
    ")\n",
    "transformer = Transformer(encoder, decoder)  \n",
    "\n",
    "transformer = Transformer(encoder, decoder)\n",
    "transformer = transformer.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(prompt, model, max_steps = 32, tokenizer = tokenizer, device = device):\n",
    "    \n",
    "    # get encoder output first\n",
    "    enc_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    enc_output = model.encode(enc_input['input_ids'], enc_input['attention_mask'])\n",
    "\n",
    "    # setup initial decoder input\n",
    "    dec_input_ids = torch.tensor([[tokenizer.cls_token_id]], device=device)\n",
    "    dec_attn_mask = torch.ones_like(dec_input_ids, device=device)\n",
    "\n",
    "    # now do the decoder loop\n",
    "    for _ in range(max_steps):\n",
    "        dec_output = model.decode(\n",
    "            enc_output,\n",
    "            dec_input_ids,\n",
    "            enc_input['attention_mask'],\n",
    "            dec_attn_mask,\n",
    "        )\n",
    "\n",
    "        # choose the best value of the last token\n",
    "        prediction_id = torch.argmax(dec_output[:, -1, :], axis=-1)\n",
    "\n",
    "        # append to decoder input\n",
    "        dec_input_ids = torch.hstack((dec_input_ids, prediction_id.view(1, 1)))\n",
    "\n",
    "        # recreate mask\n",
    "        dec_attn_mask = torch.ones_like(dec_input_ids)\n",
    "\n",
    "        # exit when reach </s>\n",
    "        if prediction_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    translation = tokenizer.decode(dec_input_ids[0, 1:])\n",
    "    return translation\n",
    "\n",
    "translate('I like to eat apples.', transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, epochs, valid_loader = None, print_every = 1):\n",
    "    def preprocess_batch(batch):\n",
    "        \"\"\"\n",
    "            This function preprocess the batch so that it can be fed to the model.\n",
    "            It is quite complex and used in both training and validation, so we\n",
    "            define it here.        \n",
    "            Note - another option is to create both dec_inputs and dec_masks \n",
    "            in the dataloader, and not to do it here. This is the traedeof\n",
    "            between speed and memory.\n",
    "        \"\"\"\n",
    "        enc_input = batch['input_ids']\n",
    "        enc_mask = batch['attention_mask']\n",
    "        targets = batch['labels']\n",
    "         \n",
    "        dec_input = targets.clone().detach()\n",
    "\n",
    "        dec_input = torch.roll(dec_input, shifts=1, dims = 1)\n",
    "        dec_input[:, 0] = tokenizer.cls_token_id\n",
    "        \n",
    "        # We cannot pass -100 in the model, as this cannot go to the \n",
    "        # embedding matrix (it's out of bounds - embeddings are 0-vocab_size). \n",
    "        # We replace it by pad_token_id\n",
    "        dec_input = dec_input.masked_fill(dec_input == -100, tokenizer.pad_token_id)\n",
    "        \n",
    "        # Creating decoder mask\n",
    "        dec_mask = torch.ones_like(dec_input)\n",
    "        dec_mask = dec_mask.masked_fill(dec_input == tokenizer.pad_token_id, 0)\n",
    "        # Note - another option is to create both dec_inputs and dec_masks \n",
    "        # in the dataloader, and not to do it here. This is the traedeof\n",
    "        # between speed and memory. \n",
    "        return enc_input, enc_mask, dec_input, dec_mask, targets\n",
    "    \n",
    "    train_losses = np.zeros(epochs)\n",
    "    valid_losses = np.zeros(epochs)\n",
    "    \n",
    "    for epoch in range (epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        print ('Training...    \\r', end = '')\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            enc_input, enc_mask, dec_input, dec_mask, targets = preprocess_batch(batch)\n",
    "            \n",
    "            \n",
    "            outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "            \n",
    "            # This part is tricky. Our output shape is N x T x V, \n",
    "            # where N is batch size, T is sequence length, and V is vocab size,\n",
    "            # and our targets shape is N x T.\n",
    "            # CrossEntropyLoss expects scores in the form \n",
    "            # N x V x T, so we need to transpose     \n",
    "            loss = criterion(outputs.transpose(2, 1), targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "\n",
    "        if valid_loader is not None:\n",
    "            print ('Validating...    \\r', end = '')\n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            for batch in valid_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                enc_input = batch['input_ids']\n",
    "                enc_mask = batch['attention_mask']\n",
    "                targets = batch['labels']\n",
    "                \n",
    "                \n",
    "                enc_input, enc_mask, dec_input, dec_mask, targets = preprocess_batch(batch)\n",
    "                \n",
    "                outputs = model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "                loss = criterion(outputs.transpose(2, 1), targets)\n",
    "                valid_loss.append(loss.item())\n",
    "            \n",
    "            valid_loss = np.mean(valid_loss)\n",
    "        else:\n",
    "            valid_loss = np.nan\n",
    "        \n",
    "        train_losses[epoch] = train_loss\n",
    "        valid_losses[epoch] = valid_loss\n",
    "\n",
    "        translation = translate(\n",
    "            prompt = \"I am ready\", \n",
    "            model = model, \n",
    "            max_steps = 10,\n",
    "            tokenizer = tokenizer, \n",
    "            device = device\n",
    "            )\n",
    "\n",
    "        \n",
    "        if epoch%print_every == 0:\n",
    "            t1 = datetime.now() - t0\n",
    "            minutes, seconds = divmod(t1.total_seconds(), 60)\n",
    "            formatted_time = \"{:02}:{:02}\".format(int(minutes), int(seconds))\n",
    "            \n",
    "            \n",
    "            print (f'Epoch: {epoch}: Train loss: {train_loss:.2f}, Valid loss: {valid_loss:.2f}, Duration: {formatted_time}min, Text: {translation}')\n",
    "           \n",
    "           \n",
    "    return train_losses, valid_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses = train(\n",
    "    model=transformer, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer,   \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
